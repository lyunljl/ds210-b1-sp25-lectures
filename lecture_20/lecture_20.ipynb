{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>DS-210: Programming for Data Science</h1>\n",
    "    <h1>Lecture 20</h1>\n",
    "</div>\n",
    "\n",
    "Today's Topics:\n",
    "\n",
    "* NDArray:  Rust's answer to numpy\n",
    "* A simple Neural Network using NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDArray\n",
    "\n",
    "* An $n$-dimensional (e.g. 1-D, 2-D, 3-D, ...) container for general elements and numerics.\n",
    "\n",
    "* Similar to Python's Numpy but with important differences.\n",
    "\n",
    "* Rich set of methods for creating views and slices into the array\n",
    "\n",
    "* Provides a rich set of methods and mathematical operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key similarities with Python Numpy (from [link](https://docs.rs/ndarray/latest/ndarray/doc/ndarray_for_numpy_users/index.html#similarities))\n",
    "\n",
    "* Arrays have single element types\n",
    "* Arrays can have arbitrary dimensions\n",
    "* Arrays can have arbitrary strides\n",
    "* Indexing starts at 0\n",
    "* Default ordering is row-major (more on that below)\n",
    "* Arithmetic operators (+, -, \\*, /) perform element-wise operations\n",
    "* Arrays that are not views are contiguous in memory\n",
    "* Many cheap operations that return views into the array instead of copying data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Some important differences from Numpy\n",
    "\n",
    "* Numpy arrays and views are all mutable and can all change the contents of an array.  \n",
    "* NDarrays have:\n",
    "    * flavors that can change contents, \n",
    "    * flavors that cannot, and \n",
    "    * flavors that make copies when things change.\n",
    "* Numpy arrays are always dynamic in their number of dimensions.  NDarrays can be static or dynamic.\n",
    "* Slices with negative steps behave differently (more on that later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## To use it\n",
    "\n",
    "To add latest `ndarray` crate.\n",
    "\n",
    "```sh\n",
    "% cargo add ndarray\n",
    "```\n",
    "\n",
    "or manually add\n",
    "\n",
    "```\n",
    "[dependencies]  \n",
    "ndarray=\"0.15.6\"  \n",
    "```\n",
    "\n",
    "or later in your Cargo.toml file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use NDarray over Vec or array?\n",
    "\n",
    "It is easier to do a bunch of things like:\n",
    "\n",
    "* Data Cleaning and Preprocessing offering functions like slicing and fill\n",
    "* Statistics built in (lots of math functions come with it)\n",
    "* Machine learning: Used in many of the ML libraries written in Rust\n",
    "* Linear Algebra: Built in methods like matrix inversion, multiplication and decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some example usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the array with debug formatting:\n",
      "[[1.0, 2.0, 3.0],\n",
      " [4.0, 5.0, 6.0]], shape=[2, 3], strides=[3, 1], layout=Cc (0x5), const ndim=2\n",
      "\n",
      "Print the array with display formatting:\n",
      "[[1, 2, 3],\n",
      " [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "// This is required in a Jupyter notebook.\n",
    "// For a cargo project, you would add it to your Cargo.toml file.\n",
    ":dep ndarray = { version = \"^0.15.6\" }\n",
    "\n",
    "use ndarray::prelude::*;\n",
    "\n",
    "fn main() {\n",
    "    let a = array![         // handy macro for creating arrays\n",
    "                [1.,2.,3.], \n",
    "                [4.,5.,6.],\n",
    "            ]; \n",
    "    assert_eq!(a.ndim(), 2);         // get the number of dimensions of array a\n",
    "    assert_eq!(a.len(), 6);          // get the number of elements in array a\n",
    "    assert_eq!(a.shape(), [2, 3]);   // get the shape of array a\n",
    "    assert_eq!(a.is_empty(), false); // check if the array has zero elements\n",
    "\n",
    "    println!(\"Print the array with debug formatting:\");\n",
    "    println!(\"{:?}\", a);\n",
    "\n",
    "    println!(\"\\nPrint the array with display formatting:\");\n",
    "    println!(\"{}\", a);\n",
    "}\n",
    "\n",
    "main();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a side by side comparison NumPy, see https://docs.rs/ndarray/latest/ndarray/doc/ndarray_for_numpy_users/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Array Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|NumPy | ndarray | Notes |\n",
    "|:-:|:-:|:-:|\n",
    "| np.array([[1.,2.,3.], [4.,5.,6.]])| array![[1.,2.,3.], [4.,5.,6.]] or arr2(&[[1.,2.,3.], [4.,5.,6.]]) | 2×3 floating-point array literal|\n",
    "|np.arange(0., 10., 0.5) | Array::range(0., 10., 0.5) | create a 1-D array with values 0., 0.5, …, 9.5|\n",
    "|np.linspace(0., 10., 11) |\tArray::linspace(0., 10., 11) | create a 1-D array with 11 elements with values 0., …, 10.|\n",
    "|np.logspace(2.0, 3.0, num=4, base=10.0) | Array::logspace(10.0, 2.0, 3.0, 4) | create a 1-D array with 4 logarithmically spaced elements with values 100., 215.4, 464.1, 1000.|\n",
    "|np.geomspace(1., 1000., num=4) | Array::geomspace(1e0, 1e3, 4) | create a 1-D array with 4 geometrically spaced elements from 1 to 1,000 inclusive: 1., 10., 100., 1000.|\n",
    "|np.ones((3, 4, 5)) | Array::ones((3, 4, 5)) | create a 3×4×5 array filled with ones (inferring the element type)|\n",
    "|np.zeros((3, 4, 5)) | Array::zeros((3, 4, 5)) | create a 3×4×5 array filled with zeros (inferring the element type)|\n",
    "|np.zeros((3, 4, 5), order='F')\t| Array::zeros((3, 4, 5).f()) | create a 3×4×5 array with Fortran (column-major) memory layout filled with zeros (inferring the element type)|\n",
    "|np.zeros_like(a, order='C') | Array::zeros(a.raw_dim()) | create an array of zeros of the shape shape as a, with row-major memory layout (unlike NumPy, this infers the element type from context instead of duplicating a’s element type)|\n",
    "|np.full((3, 4), 7.) | Array::from_elem((3, 4), 7.) | create a 3×4 array filled with the value 7.|\n",
    "|np.array([1, 2, 3, 4]).reshape((2, 2)) | Array::from_shape_vec((2, 2), vec![1, 2, 3, 4])? or .into_shape((2,2).unwrap()| create a 2×2 array from the elements in the Vec|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5], shape=[10], strides=[1], layout=CFcf (0xf), const ndim=1\n",
      "\n",
      "[[0.0, 0.5, 1.0, 1.5, 2.0],\n",
      " [2.5, 3.0, 3.5, 4.0, 4.5]], shape=[2, 5], strides=[5, 1], layout=Cc (0x5), const ndim=2\n",
      "\n",
      "[[0.0, 0.0, 0.0, 0.0],\n",
      " [0.0, 0.0, 0.0, 0.0],\n",
      " [0.0, 0.0, 0.0, 0.0]], shape=[3, 4], strides=[1, 3], layout=Ff (0xa), const ndim=2\n"
     ]
    }
   ],
   "source": [
    "let a:Array<f64, Ix1> = Array::linspace(0., 4.5, 10);\n",
    "println!(\"{:?}\", a);\n",
    "\n",
    "let b:Array<f64, _> = a.into_shape((2,5)).unwrap();\n",
    "println!(\"\\n{:?}\", b);\n",
    "\n",
    "let c:Array<f64, _> = Array::zeros((3,4).f());\n",
    "println!(\"\\n{:?}\", c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good overview at this [TDS Post](https://towardsdatascience.com/the-ultimate-ndarray-handbook-mastering-the-art-of-scientific-computing-with-rust-ef5ab767212a/#672d), but subscription required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0]], shape=[1, 4], strides=[1, 1], layout=CFcf (0xf), const ndim=2\n",
      "[[1.0, 1.0, 1.0, 1.0]], shape=[1, 4], strides=[4, 1], layout=CFcf (0xf), const ndim=2\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0], shape=[5], strides=[1], layout=CFcf (0xf), const ndim=1\n",
      "[0.0, 1.25, 2.5, 3.75, 5.0], shape=[5], strides=[1], layout=CFcf (0xf), const ndim=1\n",
      "[[0.0, 0.0, 0.0, 0.0]], shape=[1, 4], strides=[4, 1], layout=CFcf (0xf), const ndim=2\n",
      "[[1.0, 0.0, 0.0, 0.0],\n",
      " [0.0, 1.0, 0.0, 0.0],\n",
      " [0.0, 0.0, 1.0, 0.0],\n",
      " [0.0, 0.0, 0.0, 1.0]], shape=[4, 4], strides=[4, 1], layout=Cc (0x5), const ndim=2\n"
     ]
    }
   ],
   "source": [
    ":dep ndarray = { version = \"^0.15.6\" }\n",
    "\n",
    "// Not working on Jupyter notebook\n",
    "//:dep ndarray-rand = { version = \"^0.15.0\" }\n",
    "\n",
    "use ndarray::{Array, ShapeBuilder};\n",
    "\n",
    "// Not working on Jupyter notebook\n",
    "//use ndarray_rand::RandomExt;\n",
    "//use ndarray_rand::rand_distr::Uniform;\n",
    "\n",
    "\n",
    "// Ones\n",
    "\n",
    "let ones = Array::<f64, _>::ones((1, 4));\n",
    "println!(\"{:?}\", ones); \n",
    "\n",
    "// Output:\n",
    "// [[1.0, 1.0, 1.0, 1.0]], shape=[1, 4], strides=[4, 1], layout=CFcf (0xf), const ndim=2\n",
    "\n",
    "// Range\n",
    "\n",
    "let range = Array::<f64, _>::range(0., 5., 1.);\n",
    "println!(\"{:?}\", range); \n",
    "\n",
    "// Output:\n",
    "// [0.0, 1.0, 2.0, 3.0, 4.0], shape=[5], strides=[1], layout=CFcf (0xf), const ndim=1\n",
    "\n",
    "// Linspace\n",
    "\n",
    "let linspace = Array::<f64, _>::linspace(0., 5., 5);\n",
    "println!(\"{:?}\", linspace); \n",
    "\n",
    "// Output:\n",
    "// [0.0, 1.25, 2.5, 3.75, 5.0], shape=[5], strides=[1], layout=CFcf (0xf), const ndim=1\n",
    "\n",
    "// Fill\n",
    "\n",
    "let mut ones = Array::<f64, _>::ones((1, 4));\n",
    "ones.fill(0.);\n",
    "println!(\"{:?}\", ones); \n",
    "\n",
    "// Output:\n",
    "// [[0.0, 0.0, 0.0, 0.0]], shape=[1, 4], strides=[4, 1], layout=CFcf (0xf), const ndim=2\n",
    "\n",
    "// Eye -- Identity Matrix\n",
    "\n",
    "let eye = Array::<f64, _>::eye(4);\n",
    "println!(\"{:?}\", eye); \n",
    "\n",
    "// Output:\n",
    "// [[1.0, 0.0, 0.0, 0.0],\n",
    "// [0.0, 1.0, 0.0, 0.0],\n",
    "// [0.0, 0.0, 1.0, 0.0],\n",
    "// [0.0, 0.0, 0.0, 1.0]], shape=[4, 4], strides=[4, 1], layout=Cc (0x5), const ndim=2\n",
    "\n",
    "// Random\n",
    "\n",
    "// Not working on Jupyter notebook\n",
    "\n",
    "//let random = Array::random((2, 5), Uniform::new(0., 10.));\n",
    "//println!(\"{:?}\", random);\n",
    "\n",
    "// Output:\n",
    "// [[9.375493735188611, 4.088737328406999, 9.778579742815943, 0.5225866490310649, 1.518053969762827],\n",
    "//  [9.860829919571666, 2.9473768443117, 7.768332993584486, 7.163926861520167, 9.814750664983297]], shape=[2, 5], strides=[5, 1], layout=Cc (0x5), const ndim=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the printout mean?\n",
    "\n",
    "* The values of the array\n",
    "* The shape of the array, most important dimension first\n",
    "* The stride (always 1 for arrays in the last dimension, but can be different for views)\n",
    "* The layout (storage order, view order)\n",
    "* The number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Slicing\n",
    "\n",
    "|NumPy | ndarray | Notes|\n",
    "|:-:|:-:|:-:|\n",
    "|a[-1] | a[a.len() - 1] | access the last element in 1-D array a|\n",
    "|a[1, 4] | a[[1, 4]] | access the element in row 1, column 4|\n",
    "|a[1] or a[1, :, :] | a.slice(s![1, .., ..]) or a.index_axis(Axis(0), 1) | get a 2-D subview of a 3-D array at index 1 of axis 0|\n",
    "|a[0:5] or a[:5] or a[0:5, :] | a.slice(s![0..5, ..]) or a.slice(s![..5, ..]) or a.slice_axis(Axis(0), Slice::from(0..5)) | get the first 5 rows of a 2-D array|\n",
    "|a[-5:] or a[-5:, :] | a.slice(s![-5.., ..]) or a.slice_axis(Axis(0), Slice::from(-5..)) | get the last 5 rows of a 2-D array|\n",
    "|a[:3, 4:9]\t| a.slice(s![..3, 4..9]) | columns 4, 5, 6, 7, and 8 of the first 3 rows|\n",
    "|a[1:4:2, ::-1]\t| a.slice(s![1..4;2, ..;-1]) | rows 1 and 3 with the columns in reverse order|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `s![]` slice macro\n",
    "\n",
    "* The `s![]` macro in Rust's ndarray crate is a convenient way to create slice specifications for array operations. \n",
    "\n",
    "* It's used to create a `SliceInfo` object that describes how to slice or view an array.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. The `s![]` macro is used to create slice specifications that are similar to Python's slice notation\n",
    "2. It's commonly used with methods like `slice()`, `slice_mut()`, and other array view operations\n",
    "3. Inside the macro, you can specify ranges and steps for each dimension\n",
    "\n",
    "For example:\n",
    "\n",
    "```rust\n",
    "let mut slice = array.slice_mut(s![1.., 0, ..]);\n",
    "```\n",
    "\n",
    "This creates a slice that:\n",
    "- Takes all elements from index 1 onwards in the first dimension (`1..`)\n",
    "- Takes only index 0 in the second dimension (`0`)\n",
    "- Takes all elements in the third dimension (`..`)\n",
    "\n",
    "The syntax inside `s![]` supports several patterns:\n",
    "- `..` - take all elements in that dimension\n",
    "- `start..end` - take elements from start (inclusive) to end (exclusive)\n",
    "- `start..=end` - take elements from start (inclusive) to end (inclusive)\n",
    "- `start..` - take elements from start (inclusive) to the end\n",
    "- `..end` - take elements from the beginning to end (exclusive)\n",
    "- `index` - take only that specific index\n",
    "\n",
    "For example:\n",
    "```rust\n",
    "// Take every other element in the first dimension\n",
    "s![..;2]\n",
    "\n",
    "// Take elements 1 to 3 in the first dimension, all elements in the second\n",
    "s![1..4, ..]\n",
    "\n",
    "// Take elements from index 2 to the end, with step size 2\n",
    "s![2..;2]\n",
    "\n",
    "// Take specific indices\n",
    "s![1, 2, 3]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"figs/tds_array_slicing.webp\" width=\"40%\">\n",
    "</div>\n",
    "\n",
    "From [TDS Post](https://towardsdatascience.com/the-ultimate-ndarray-handbook-mastering-the-art-of-scientific-computing-with-rust-ef5ab767212a/#672d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 3D array:\n",
      "[[[0.0, 1.0, 2.0, 3.0],\n",
      "  [10.0, 11.0, 12.0, 13.0],\n",
      "  [20.0, 21.0, 22.0, 23.0]],\n",
      "\n",
      " [[100.0, 101.0, 102.0, 103.0],\n",
      "  [110.0, 111.0, 112.0, 113.0],\n",
      "  [120.0, 121.0, 122.0, 123.0]]], shape=[2, 3, 4], strides=[12, 4, 1], layout=Cc (0x5), const ndim=3\n",
      "2D slice:\n",
      "[[100.0, 101.0, 102.0, 103.0]], shape=[1, 4], strides=[0, 1], layout=CFcf (0xf), const ndim=2\n",
      "1D slice:\n",
      "[100.0, 101.0, 102.0, 103.0], shape=[4], strides=[1], layout=CFcf (0xf), const ndim=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use ndarray::{s};\n",
    "{\n",
    "    // Create a 3-dimensional array (2x3x4)\n",
    "    let mut array = Array::from_shape_fn((2, 3, 4), |(i, j, k)| {\n",
    "        (i * 100 + j * 10 + k) as f32\n",
    "    });\n",
    "\n",
    "    // Print the original 3-dimensional array\n",
    "    println!(\"Original 3D array:\\n{:?}\", array);\n",
    "\n",
    "    // Create a 2-dimensional slice (taking the first 2D layer)\n",
    "    let mut slice = array.slice_mut(s![1.., 0, ..]);\n",
    "\n",
    "    // Print the 2-dimensional slice\n",
    "    println!(\"2D slice:\\n{:?}\", slice);\n",
    "\n",
    "    // Create a 1-dimensional slice (taking the first 2D and 3D layer)\n",
    "    let slice2 = array.slice(s![1, 0, ..]);\n",
    "\n",
    "    // Print the 1-dimensional slice\n",
    "    println!(\"1D slice:\\n{:?}\", slice2);\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D and 3D datatypes, again from [TDS Post](https://towardsdatascience.com/the-ultimate-ndarray-handbook-mastering-the-art-of-scientific-computing-with-rust-ef5ab767212a/#672d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 3.0, 4.0], shape=[4], strides=[1], layout=CFcf (0xf), const ndim=1\n",
      "[[1.0, 2.0, 3.0, 4.0]], shape=[1, 4], strides=[4, 1], layout=CFcf (0xf), const ndim=2\n",
      "[[1.0, 2.0],\n",
      " [3.0, 4.0]], shape=[2, 2], strides=[2, 1], layout=Cc (0x5), const ndim=2\n",
      "Ok([[[1.0],\n",
      "  [2.0]],\n",
      "\n",
      " [[3.0],\n",
      "  [4.0]]], shape=[2, 2, 1], strides=[2, 1, 1], layout=Cc (0x5), const ndim=3)\n"
     ]
    }
   ],
   "source": [
    "use ndarray::{array, Array, Array2, Array3, ShapeBuilder};\n",
    "\n",
    "// 1D array\n",
    "let array_d1 = Array::from_vec(vec![1., 2., 3., 4.]);\n",
    "println!(\"{:?}\", array_d1);\n",
    "\n",
    "// Output:\n",
    "// [1.0, 2.0, 3.0, 4.0], shape=[4], strides=[1], layout=CFcf (0xf), const ndim=1\n",
    "\n",
    "// or \n",
    "\n",
    "let array_d11 = Array::from_shape_vec((1, 4), vec![1., 2., 3., 4.]);\n",
    "println!(\"{:?}\", array_d11.unwrap());\n",
    "\n",
    "// Output:\n",
    "// [[1.0, 2.0, 3.0, 4.0]], shape=[1, 4], strides=[4, 1], layout=CFcf (0xf), const ndim=2\n",
    "\n",
    "// 2D array\n",
    "\n",
    "let array_d2 = array![\n",
    "    [-1.01,  0.86, -4.60,  3.31, -4.81],\n",
    "    [ 3.98,  0.53, -7.04,  5.29,  3.55],\n",
    "    [ 3.30,  8.26, -3.89,  8.20, -1.51],\n",
    "    [ 4.43,  4.96, -7.66, -7.33,  6.18],\n",
    "    [ 7.31, -6.43, -6.16,  2.47,  5.58],\n",
    "];\n",
    "\n",
    "// or\n",
    "\n",
    "let array_d2 = Array::from_shape_vec((2, 2), vec![1., 2., 3., 4.]);\n",
    "println!(\"{:?}\", array_d2.unwrap());\n",
    "\n",
    "// Output:\n",
    "// [[1.0, 2.0],\n",
    "// [3.0, 4.0]], shape=[2, 2], strides=[2, 1], layout=Cc (0x5), const ndim=2\n",
    "\n",
    "// or\n",
    "\n",
    "let mut data = vec![1., 2., 3., 4.];\n",
    "let array_d21 = Array2::from_shape_vec((2, 2), data);\n",
    "\n",
    "// 3D array\n",
    "\n",
    "let mut data = vec![1., 2., 3., 4.];\n",
    "let array_d3 = Array3::from_shape_vec((2, 2, 1), data);\n",
    "println!(\"{:?}\", array_d3);\n",
    "\n",
    "// Output:\n",
    "// [[[1.0],\n",
    "//  [2.0]],\n",
    "//  [[3.0],\n",
    "//  [4.0]]], shape=[2, 2, 1], strides=[2, 1, 1], layout=Cc (0x5), const ndim=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figs/tds_reshape.webp\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "From [TDS Post](https://towardsdatascience.com/the-ultimate-ndarray-handbook-mastering-the-art-of-scientific-computing-with-rust-ef5ab767212a/#672d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"figs/tds_flatten.webp\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "From [TDS Post](https://towardsdatascience.com/the-ultimate-ndarray-handbook-mastering-the-art-of-scientific-computing-with-rust-ef5ab767212a/#672d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the array parameters\n",
    "\n",
    "|NumPy | ndarray | Notes|\n",
    "|:-:|:-:|:-:|\n",
    "|np.ndim(a) or a.ndim | a.ndim() | get the number of dimensions of array a|\n",
    "|np.size(a) or a.size | a.len()\t| get the number of elements in array a|\n",
    "|np.shape(a) or a.shape\t| a.shape() or a.dim() | get the shape of array a|\n",
    "|a.shape[axis] | a.len_of(Axis(axis)) | get the length of an axis|\n",
    "|a.strides | a.strides() | get the strides of array a|\n",
    "|np.size(a) == 0 or a.size == 0\t| a.is_empty() | check if the array has zero elements|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Math\n",
    "\n",
    "|NumPy | ndarray | Notes|\n",
    "|:-:|:-:|:-:|\n",
    "|a.transpose() or a.T | a.t() or a.reversed_axes() | transpose of array a (view for .t() or by-move for .reversed_axes())|\n",
    "|mat1.dot(mat2) | mat1.dot(&mat2) |2-D matrix multiply|\n",
    "|mat.dot(vec) | mat.dot(&vec) | 2-D matrix dot 1-D column vector|\n",
    "|vec.dot(mat) | vec.dot(&mat) | 1-D row vector dot 2-D matrix |\n",
    "|vec1.dot(vec2) | vec1.dot(&vec2) | vector dot product|\n",
    "|a * b, a + b, etc. | a * b, a + b, etc. | element-wise arithmetic operations|\n",
    "|a\\**3 | a.mapv(\\|a\\| a.powi(3)) | element-wise power of 3|\n",
    "|np.sqrt(a) | a.mapv(f64::sqrt) | element-wise square root for f64 array |\n",
    "|(a>0.5) | a.mapv(\\|a\\| a > 0.5)| array of bools of same shape as a with true where a > 0.5 and false elsewhere|\n",
    "|np.sum(a) or a.sum() | a.sum() | sum the elements in a|\n",
    "| np.sum(a, axis=2) or a.sum(axis=2) | a.sum_axis(Axis(2)) | sum the elements in a along axis 2|\n",
    "|np.mean(a) or a.mean() | a.mean().unwrap() | calculate the mean of the elements in f64 array a|\n",
    "|np.mean(a, axis=2) or a.mean(axis=2) | a.mean_axis(Axis(2)) | calculate the mean of the elements in a along axis 2|\n",
    "|np.allclose(a, b, atol=1e-8) | a.abs_diff_eq(&b, 1e-8) |  check if the arrays’ elementwise differences are within an absolute tolerance (it requires the approx feature-flag)|\n",
    "| np.diag(a) | a.diag() | view the diagonal of a|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapv vs map:  mapv iterates over the values of the array, map iterates over mutable references of the array\n",
    "\n",
    "### Array0 to Array6 also defined in addition to Array as special cases with fixed dimensions instead dynamically defined dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2D array:\n",
      "[[0.0, 1.0, 2.0, 3.0],\n",
      " [10.0, 11.0, 12.0, 13.0],\n",
      " [20.0, 21.0, 22.0, 23.0]], shape=[3, 4], strides=[4, 1], layout=Cc (0x5), const ndim=2\n",
      "\n",
      "b:\n",
      "[[0.0, 2.0, 4.0, 6.0],\n",
      " [20.0, 22.0, 24.0, 26.0],\n",
      " [40.0, 42.0, 44.0, 46.0]], shape=[3, 4], strides=[4, 1], layout=Cc (0x5), const ndim=2\n",
      "\n",
      "c:\n",
      "[[false, false, false, false],\n",
      " [false, false, false, true],\n",
      " [true, true, true, true]], shape=[3, 4], strides=[4, 1], layout=Cc (0x5), const ndim=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    // Create a 2-dimensional array (3x4)\n",
    "    let mut a = Array::from_shape_fn((3, 4), |(j, k)| {\n",
    "        (j * 10 + k) as f32\n",
    "    });\n",
    "\n",
    "    // Print the original 2-dimensional array\n",
    "    println!(\"Original 2D array:\\n{:?}\", a);\n",
    "\n",
    "    let b = a * 2.0;\n",
    "    println!(\"\\nb:\\n{:?}\", b);\n",
    "\n",
    "    let c = b.mapv(|v| v>24.8);\n",
    "    println!(\"\\nc:\\n{:?}\", c);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Conversions\n",
    "\n",
    "* std::convert::From ensures lossless, safe conversions at compile-time and is generally recommended.\n",
    "* std::convert::TryFrom can be used for potentially unsafe conversions. It will return a Result which can be handled or unwrap()ed to panic if any value at runtime cannot be converted losslessly.\n",
    "\n",
    "\n",
    "|NumPy | ndarray | Notes|\n",
    "|:-:|:-:|:-:|\n",
    "|a.astype(np.float32) | a.mapv(\\|x\\| f32::from(x)) | convert array to f32.  Only use if can't fail |\n",
    "|a.astype(np.int32) | a.mapv(\\|x\\| i32::from(x)) | convert array to i32.  Only use if can't fail |\n",
    "|a.astype(np.uint8) | a.mapv(\\|x\\| u8::try_from(x).unwrap()) | try to convert to u8 array, panic if any value cannot be converted lossless at runtime (e.g. negative value)|\n",
    "|a.astype(np.int32) | a.mapv(\\|x\\| x as i32) | convert to i32 array with “saturating” conversion; care needed because it can be a lossy conversion or result in non-finite values!|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Linear Algebra\n",
    "\n",
    "Provided by `NDArray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figs/tds_transpose.webp\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let array_d2 = Array::from_shape_vec((2, 2), vec![1., 2., 3., 4.]);\n",
    "println!(\"{:?}\", array_d2.unwrap());\n",
    "\n",
    "// Output\n",
    "// [[1.0, 2.0],\n",
    "//  [3.0, 4.0]], shape=[2, 2], strides=[2, 1], layout=Cc (0x5), const ndim=2)\n",
    "\n",
    "let binding = array_d2.expect(\"Expect 2d matrix\");\n",
    "\n",
    "let array_d2t = binding.t();\n",
    "println!(\"{:?}\", array_d2t);\n",
    "\n",
    "// Output\n",
    "// [[1.0, 3.0],\n",
    "//  [2.0, 4.0]], shape=[2, 2], strides=[1, 2], layout=Ff (0xa), const ndim=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [2.0, 8.0]], shape=[2, 2], strides=[2, 1], layout=Cc (0x5), const ndim=2[[13.0, 2.0],\n"
     ]
    }
   ],
   "source": [
    "use ndarray::{array, Array2};\n",
    "\n",
    "let a: Array2<f64> = array![[3., 2.], [2., -2.]];\n",
    "let b: Array2<f64> = array![[3., 2.], [2., -2.]];\n",
    "let c = a.dot(&b);\n",
    "print!(\"{:?}\", c);\n",
    "\n",
    "// Output\n",
    "// [[13.0, 2.0],\n",
    "//  [2.0, 8.0]], shape=[2, 2], strides=[2, 1], layout=Cc (0x5), const ndim=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra with `ndarray-linalg`\n",
    "\n",
    "* The crate [`ndarray-linalg`](https://crates.io/crates/ndarray-linalg) implements more advanced lineary algebra operations that comes with `NDArray`.\n",
    "\n",
    "* It relies on a native linear algebra library like `OpenBLAS` and can be tricky to configure.\n",
    "\n",
    "* We show it here just for reference.\n",
    "\n",
    "|NumPy | ndarray | Notes|\n",
    "|:-:|:-:|:-:|\n",
    "|numpy.linalg.inv(a) | a.inv() | Invert matrix a.  Must be square|\n",
    "|numpy.linalg.eig(a) | a.eig() | Compute eigenvalues and eigenvectors of matrix.  Must be square|\n",
    "|numpy.linalg.svd(a) | a.svd(true, true) | Compute the Singular Value Decomposition of matrix|\n",
    "|numpy.linalg.det(a) | a.det() | Compute the determinant of a matrix. Must be square|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The type of the variable c was redefined, so was lost.\n",
      "The type of the variable b was redefined, so was lost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      "[Complex { re: 7.256022422687388, im: 0.0 }, Complex { re: -0.026352822204034426, im: 0.0 }, Complex { re: -5.229669600483354, im: 0.0 }], shape=[3], strides=[1], layout=CFcf (0xf), const ndim=1\n",
      "Eigenvectors:\n",
      "[[Complex { re: -0.4992701697014973, im: 0.0 }, Complex { re: -0.7576983872742932, im: 0.0 }, Complex { re: -0.22578016277159085, im: 0.0 }],\n",
      " [Complex { re: -0.4667420094775666, im: 0.0 }, Complex { re: 0.6321277120502754, im: 0.0 }, Complex { re: -0.526348454767688, im: 0.0 }],\n",
      " [Complex { re: -0.7299871192254567, im: 0.0 }, Complex { re: -0.162196515314045, im: 0.0 }, Complex { re: 0.8197442419819131, im: 0.0 }]], shape=[3, 3], strides=[1, 3], layout=Ff (0xa), const ndim=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//```rust\n",
    ":dep ndarray = { version = \"^0.15\" }\n",
    "// This is the MAC version\n",
    "// See ./README.md for setup instructions\n",
    ":dep ndarray-linalg = { version = \"^0.16\", features = [\"openblas-system\"] }\n",
    "// This is the linux verison\n",
    "//:dep ndarray-linalg = { version = \"^0.15\", features = [\"openblas\"] }\n",
    "\n",
    "use ndarray::array;\n",
    "use ndarray_linalg::*;\n",
    "\n",
    "{\n",
    "    // Create a 2D square matrix (3x3)\n",
    "    let matrix = array![\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [0.0, 1.0, 4.0],\n",
    "        [5.0, 6.0, 0.0]\n",
    "    ];\n",
    "\n",
    "    // Compute the eigenvalues and eigenvectors\n",
    "    match matrix.eig() {\n",
    "        Ok((eigenvalues, eigenvectors)) => {\n",
    "            println!(\"Eigenvalues:\\n{:?}\", eigenvalues);\n",
    "            println!(\"Eigenvectors:\\n{:?}\", eigenvectors);\n",
    "        },\n",
    "        Err(e) => println!(\"Failed to compute eigenvalues and eigenvectors: {:?}\", e),\n",
    "    }\n",
    "}\n",
    "//```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same code is provided as a [cargo project](./neural_network/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Numpy Reference\n",
    "\n",
    "For reference you can look at [mnist_fcn.ipynb](./mnist_fcn.ipynb) which implements and \n",
    "trains the network with only numpy matrices, but does use PyTorch dataset loaders for conciseness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDArray and Neural networks\n",
    "\n",
    "* Simple neural networks can be easily represented by matrix arithmentic.\n",
    "\n",
    "* Let's see how we can build a relatively simple neural network to recognize handwritten digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neuron\n",
    "\n",
    "An _artificial neuron_is loosely modeled on a biological neuron.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./figs/neuron.png\" width=\"75%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "From Stanford's [cs231n](https://cs231n.github.io/neural-networks-1/)\n",
    "\n",
    "* The dendrites carry impulses from other neurons of different distances.\n",
    "* Once the collective firing rate of the impulses exceed a certain threshold,\n",
    "  the neuron fires its own pulse through the axon to other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neuron\n",
    "\n",
    "The artificial neuron is loosely based on the biological one.\n",
    "\n",
    "<div>\n",
    "    <img src=\"figs/neuron_model.jpeg\" width=\"75%\">\n",
    "</div>\n",
    "\n",
    "From Stanford's [cs231n](https://cs231n.github.io/neural-networks-1/).\n",
    "\n",
    "The artifical neuron:\n",
    "\n",
    "* collects one or more inputs, \n",
    "* each multiplied by a unique weight,\n",
    "* sums the weighted inputs,\n",
    "* adds a bias,\n",
    "* then applies a nonlinear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) or Fully Connected Network (FCN)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"figs/neural_net2.jpeg\" width=\"75%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "From Stanford's [cs231n](https://cs231n.github.io/convolutional-networks/).\n",
    "\n",
    "* Multiple artificial neurons can act on the same inputs,. This defines\n",
    "a _layer_. We can have more than one _layer_ until we produce one or more\n",
    "outputs.\n",
    "\n",
    "* The example above shows a network with _3 inputs_, two layers of neurons, each\n",
    "with 4 neurons, followed by one layer that produces a single value output.\n",
    "\n",
    "This architecture can be used as a binary classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN Detailed View\n",
    "\n",
    "Here's a more detailed view of a fully connected network making biases explicit and with weight matrix and bias vector dimensions\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./figs/udl_fcn_matrices.png\" width=\"60%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can collect all the inputs into a column vector.\n",
    "\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we can gather the weights for the first layer:\n",
    "\n",
    "$$\n",
    "h_1 = \n",
    "\\textrm{a} \\left[\n",
    "\\mathbf{\\Omega}_0 \\mathbf{x} + \\boldsymbol{\\beta}_0 \\right] =\n",
    "\\textrm{a}\\left[\n",
    "\\begin{bmatrix}\n",
    "\\omega_{11} & \\omega_{12} & \\omega_{13} \\\\\n",
    "\\omega_{21} & \\omega_{22} & \\omega_{23} \\\\\n",
    "\\omega_{31} & \\omega_{32} & \\omega_{33} \\\\\n",
    "\\omega_{41} & \\omega_{42} & \\omega_{43} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\beta_3 \\\\\n",
    "\\beta_4\n",
    "\\end{bmatrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where $a[z]$ is a nonlinear activation function, such as ReLU shown below.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figs/ReLU.png\" width=\"75%\">\n",
    "</div>\n",
    "\n",
    "The equation for which is.\n",
    "\n",
    "$$\n",
    "a[z] = \\text{ReLU}[z] = \n",
    "\\begin{cases}\n",
    "0 & z < 0 \\\\\n",
    "z & z \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "or in pseudocode simply:\n",
    "\n",
    "```python\n",
    "x = max(0, x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The nonlinear activation functions are crucial, because without them, the neural\n",
    "network can be simplified down to a single linear system of equations, which is\n",
    "severely limited in its representational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsequent layers can be defined similarly interms of the activations of the\n",
    "previous activation functions.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}_2 &= \\textrm{a} \\left[\\mathbf{\\Omega}_1 \\mathbf{h}_1 + \\boldsymbol{\\beta} \\right] \\\\\n",
    "\\mathbf{h}_3 &= \\textrm{a} \\left[\\mathbf{\\Omega}_2 \\mathbf{h}_2 + \\boldsymbol{\\beta} \\right] \\\\\n",
    "\\mathbf{h}_4 &= \\mathbf{\\Omega}_3 \\mathbf{h}_3 + \\boldsymbol{\\beta} \\\\\n",
    "\\mathbf{y} &= \\textrm{softmax} \\left[ \\mathbf{h}_4 \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the last function is a softmax, which limits the output to between 0 and 1,\n",
    "and all the outputs sum up to 1.\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}^K e^{y_j}}\n",
    "$$\n",
    "\n",
    "Here's a visualization.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figs/softmax.webp\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "From [TDS Post](https://medium.com/data-science/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Much of neural network training and evaluation comes down to vector operations in which ndarray excels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Networks\n",
    "\n",
    "* We won't go into all the details of training a network, but we'll show an example.\n",
    "\n",
    "* The steps involve producing the outpur from some input -- **Forward Pass**\n",
    "\n",
    "* Then updating the weights and biases of the network, taking the _partial derivatives_ of the entire model\n",
    "  with respect to each parameter and then updating each parameter proportional to its partial derivative -- **Backward Pass**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple weight update example\n",
    "\n",
    "We'll build our understanding of the gradient by considering derivatives of single\n",
    "variable functions.\n",
    "\n",
    "Let's start with a quadratic function\n",
    "\n",
    "$$\n",
    "f(x) = 3x^2 - 4x +5.\n",
    "$$\n",
    "\n",
    "The derivative with respect to $x$ of our example is\n",
    "\n",
    "$$\n",
    "f'(x) = 6x-4.\n",
    "$$\n",
    "\n",
    "This is the asymptotic slope at any particular value of $x$.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figs/1d_derivative_plot.png\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "**Question:** Which way does $x$ need to move at $x = -2$ to _decrease_ $f(x)$? Positive or negative direction?\n",
    "\n",
    "**Question:** What about at $x=2$? Positive or negative direction?\n",
    "<br><br><br><br>\n",
    "\n",
    "An easy way to update this is:\n",
    "\n",
    "$$\n",
    "x \\leftarrow x - \\alpha \\frac{\\partial f}{\\partial x}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is some small number (e.g. 0.01) which we call the **learning rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing the Network Loss\n",
    "\n",
    "To train our network we want to minize some function that computes the difference\n",
    "between our target values $t_i$ and what the network produces $y_i$ for each input $x_i$.\n",
    "\n",
    "An appropriate loss function for classification models is the **categorical cross-entropy loss**.\n",
    "\n",
    "The following image from the [Medium Post]() illustrates the full pipeline:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"figs/medium_softmax_loss.webp\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "except in the figure $s$ is the model output and $y$ are the targets.\n",
    "\n",
    "We won't go into details but a nice output is that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial \\mathbf{h}_4} = \\mathbf{y} - \\mathbf{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "* Weights would be represented by an `Array2<f32>` with dimensions: (# neurons $\\times$ # inputs)\n",
    "    * For example if you have 100 input neurons and 50 2nd layer neurons then you need a $50\\times 100$ matrix of weights\n",
    "\n",
    "* Bias vectors would be represented by an `Array2<f32>` with dimensions (# neurons $\\times$ 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards propagation\n",
    "\n",
    "* As mentioned, in practice the calculation of partial derivatives must be calculated starting with the end\n",
    "  of the pipeline and then working backwards.\n",
    "\n",
    "* The chain rule for differentiation in calculus gives a modular, scalable way of doing this calculation, again\n",
    "  starting from the end and working backwards.\n",
    "\n",
    "Just a reminder, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{f}_0 &= \\mathbf{\\Omega}_0 \\mathbf{x} + \\boldsymbol{\\beta}_0  \\\\\n",
    "\\mathbf{h}_1 &= \\textrm{a} \\left[ \\mathbf{f}_0 \\right] \\\\\n",
    "\\mathbf{f}_1 &= \\mathbf{\\Omega}_1 \\mathbf{h}_1 + \\boldsymbol{\\beta}  \\\\\n",
    "\\mathbf{h}_2 &= \\textrm{a} \\left[  \\mathbf{f}_1  \\right] \\\\\n",
    "\\mathbf{f}_2 &= \\mathbf{\\Omega}_2 \\mathbf{h}_2 + \\boldsymbol{\\beta} \\\\\n",
    "\\mathbf{h}_3 &= \\textrm{a} \\left[ \\mathbf{f}_2 \\right] \\\\\n",
    "\\mathbf{h}_4 &= \\mathbf{\\Omega}_3 \\mathbf{h}_3 + \\boldsymbol{\\beta} \\\\\n",
    "\\mathbf{y} &= \\textrm{softmax} \\left[ \\mathbf{h}_4 \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We already did\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell_i}{\\partial \\mathbf{h}_4} &= \\mathbf{y} - \\mathbf{t} \\\\\n",
    "\\frac{\\partial \\ell_i}{\\partial \\mathbf{\\Omega}_3} &= \\frac{\\partial \\ell_i}{\\partial \\mathbf{h}_4} \\frac{\\partial \\mathbf{h}_4}{\\partial \\mathbf{\\Omega}_3} = \\frac{\\partial \\ell_i}{\\partial \\mathbf{h}_4} \\mathbf{h}_3^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And the process continues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Implementaiton\n",
    "\n",
    "Let's look at an example implementation.\n",
    "\n",
    "We'll start with some utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    ":dep ndarray = { version = \"^0.15\" }\n",
    ":dep rand = { version = \"0.8\" }\n",
    "\n",
    "use ndarray::prelude::*;\n",
    "use rand::Rng;\n",
    "\n",
    "// Helper function to populate arrays with random values\n",
    "// We could use ndarray-rand crate to do this, but it doesn't seem to work in a Jupyter notebook\n",
    "fn populate_array(arr: &mut Array2<f32>, m: usize, n: usize) {\n",
    "    let mut rng = rand::thread_rng();\n",
    "    for i in 0..m {\n",
    "        for j in 0..n {\n",
    "            arr[(i, j)] = rng.gen_range(-1.0..1.0);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// ReLU activation function\n",
    "fn relu(x: &Array2<f32>) -> Array2<f32> {\n",
    "    x.mapv(|x| if x > 0.0 { x } else { 0.0 })\n",
    "}\n",
    "\n",
    "// Derivative of ReLU\n",
    "fn relu_derivative(x: &Array2<f32>) -> Array2<f32> {\n",
    "    x.mapv(|x| if x > 0.0 { 1.0 } else { 0.0 })\n",
    "}\n",
    "\n",
    "// Softmax function\n",
    "fn softmax(x: &Array2<f32>) -> Array2<f32> {\n",
    "    let exp_x = x.mapv(|x| x.exp());\n",
    "    let sum_exp_x = exp_x.sum();\n",
    "    exp_x / sum_exp_x\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll define a `NeuralNetwork` struct and associated methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "struct NeuralNetwork {\n",
    "    input_size: usize,\n",
    "    output_size: usize,\n",
    "    weights1: Array2<f32>,\n",
    "    biases1: Array2<f32>,\n",
    "    weights2: Array2<f32>,\n",
    "    biases2: Array2<f32>,\n",
    "    learning_rate: f32,\n",
    "}\n",
    "\n",
    "impl NeuralNetwork {\n",
    "    /// Create a shallow neural network with one hidden layer\n",
    "    /// \n",
    "    /// # Arguments\n",
    "    /// \n",
    "    /// * `input_size` - The number of input neurons\n",
    "    /// * `hidden_size` - The number of neurons in the hidden layer\n",
    "    /// * `output_size` - The number of output neurons\n",
    "    /// * `learning_rate` - The learning rate for the neural network\n",
    "    fn new(input_size: usize, hidden_size: usize, output_size: usize, learning_rate: f32) -> Self {\n",
    "        let mut weights1 = Array2::zeros((hidden_size, input_size));\n",
    "        let mut weights2 = Array2::zeros((output_size, hidden_size));\n",
    "\n",
    "        // Initialize weights with random values\n",
    "        populate_array(&mut weights1, hidden_size, input_size);\n",
    "        populate_array(&mut weights2, output_size, hidden_size);\n",
    "\n",
    "        let biases1 = Array2::zeros((hidden_size, 1));\n",
    "        let biases2 = Array2::zeros((output_size, 1));\n",
    "\n",
    "        NeuralNetwork {\n",
    "            input_size,\n",
    "            output_size,\n",
    "            weights1,\n",
    "            biases1,\n",
    "            weights2,\n",
    "            biases2,\n",
    "            learning_rate,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fn forward(&self, input: &Array2<f32>) -> (Array2<f32>, Array2<f32>, Array2<f32>) {\n",
    "        // First layer\n",
    "        let pre_activation1 = self.weights1.dot(input) + &self.biases1;\n",
    "        let hidden = relu(&pre_activation1);\n",
    "\n",
    "        // Output layer\n",
    "        let pre_activation2 = self.weights2.dot(&hidden) + &self.biases2;\n",
    "        let output = softmax(&pre_activation2);\n",
    "\n",
    "        (hidden, pre_activation2, output)\n",
    "    }\n",
    "\n",
    "    fn backward(\n",
    "        &mut self,\n",
    "        input: &Array2<f32>,\n",
    "        hidden: &Array2<f32>,\n",
    "        pre_activation2: &Array2<f32>,\n",
    "        output: &Array2<f32>,\n",
    "        target: &Array2<f32>,\n",
    "    ) {\n",
    "        let batch_size = input.shape()[1] as f32;\n",
    "\n",
    "        // Calculate gradients for output layer\n",
    "        let output_error = output - target;\n",
    "        \n",
    "        // Gradients for weights2 and biases2\n",
    "        let d_weights2 = output_error.dot(&hidden.t()) / batch_size;\n",
    "        let d_biases2 = &output_error.sum_axis(Axis(1)).insert_axis(Axis(1)) / batch_size;\n",
    "\n",
    "        // Backpropagate error to hidden layer\n",
    "        let hidden_error = self.weights2.t().dot(&output_error);\n",
    "        let hidden_gradient = &hidden_error * &relu_derivative(hidden);\n",
    "\n",
    "        // Gradients for weights1 and biases1\n",
    "        let d_weights1 = hidden_gradient.dot(&input.t()) / batch_size;\n",
    "        let d_biases1 = &hidden_gradient.sum_axis(Axis(1)).insert_axis(Axis(1)) / batch_size;\n",
    "\n",
    "        // Update weights and biases using gradient descent\n",
    "        self.weights2 = &self.weights2 - &(d_weights2 * self.learning_rate);\n",
    "        self.biases2 = &self.biases2 - &(d_biases2 * self.learning_rate);\n",
    "        self.weights1 = &self.weights1 - &(d_weights1 * self.learning_rate);\n",
    "        self.biases1 = &self.biases1 - &(d_biases1 * self.learning_rate);\n",
    "    }\n",
    "\n",
    "    fn train(&mut self, input: &Array2<f32>, target: &Array2<f32>) -> f32 {\n",
    "        // Forward pass\n",
    "        let (hidden, pre_activation2, output) = self.forward(input);\n",
    "\n",
    "        // Calculate loss (cross-entropy)\n",
    "        let epsilon = 1e-15;\n",
    "        let loss = -target * &output.mapv(|x| (x + epsilon).ln());\n",
    "        let loss = loss.sum() / (input.shape()[1] as f32);\n",
    "\n",
    "        // Backward pass\n",
    "        self.backward(input, &hidden, &pre_activation2, &output, target);\n",
    "\n",
    "        loss\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fn main() {\n",
    "    // Create a neural network\n",
    "    let mut nn = NeuralNetwork::new(6, 6, 4, 0.01);\n",
    "\n",
    "    // Create sample input\n",
    "    let mut input = Array2::zeros((nn.input_size, 1));\n",
    "    populate_array(&mut input, nn.input_size, 1);\n",
    "\n",
    "    // Create sample target\n",
    "    let mut target = Array2::zeros((nn.output_size, 1));\n",
    "    populate_array(&mut target, nn.output_size, 1);\n",
    "    \n",
    "    // Calculate initial cross entropy loss before training\n",
    "    let (_, _, initial_output) = nn.forward(&input);\n",
    "    let epsilon = 1e-15;\n",
    "    let initial_loss = -&target * &initial_output.mapv(|x| (x + epsilon).ln());\n",
    "    let initial_loss = initial_loss.sum() / (input.shape()[1] as f32);\n",
    "    println!(\"Initial loss before training: {}\", initial_loss);\n",
    "\n",
    "    \n",
    "    // Train for one iteration\n",
    "    let loss = nn.train(&input, &target);\n",
    "    //println!(\"Loss: {}\", loss);\n",
    "\n",
    "    // Calculate loss after training\n",
    "    let (_, _, final_output) = nn.forward(&input);\n",
    "    let epsilon = 1e-15;\n",
    "    let final_loss = -&target * &final_output.mapv(|x| (x + epsilon).ln());\n",
    "    let final_loss = final_loss.sum() / (input.shape()[1] as f32);\n",
    "    println!(\"loss after training: {}\", final_loss);\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run an iteration of a forward pass and backward pass.\n",
    "\n",
    "You can repeatedly run this cell to see the loss decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss before training: 1.0153453\n",
      "loss after training: 0.8079079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Has anyone implemented neural networks in Rust?\n",
    "\n",
    "See for example [candle](https://github.com/huggingface/candle). Perhaps a pun on \"Torch\" from \"PyTorch\".\n",
    "\n",
    "But the point isn't to use an existing package but learn how to build a simple one from basic linear algebra principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All of this and more...\n",
    "\n",
    "* Quick start: https://github.com/rust-ndarray/ndarray/blob/master/README-quick-start.md\n",
    "* Numpy equivalence: https://docs.rs/ndarray/latest/ndarray/doc/ndarray_for_numpy_users/index.html\n",
    "* Linear Algebra: https://github.com/rust-ndarray/ndarray-linalg/blob/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In class poll\n",
    "\n",
    "https://piazza.com/class/m5qyw6267j12cj/post/441\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
